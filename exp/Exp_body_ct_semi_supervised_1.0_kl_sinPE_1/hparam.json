{"num_training": 400, "num_val": 0, "total_epochs": 100, "log_per_epoch": null, "save_per_epoch": 10, "starting_epoch": 0, "total_iterations": null, "log_per_iteration": 10, "save_per_iteration": null, "starting_iteration": 0, "train_size": 400, "val_size": 33, "device": "cuda:3", "model_class": "<class 'model.insertnet_v1.InsertionNet'>", "model_info": {"training": true, "_parameters": {}, "_buffers": {}, "_non_persistent_buffers_set": "set()", "_backward_pre_hooks": {}, "_backward_hooks": {}, "_is_full_backward_hook": null, "_forward_hooks": {}, "_forward_hooks_with_kwargs": {}, "_forward_hooks_always_called": {}, "_forward_pre_hooks": {}, "_forward_pre_hooks_with_kwargs": {}, "_state_dict_hooks": {}, "_state_dict_pre_hooks": {}, "_load_state_dict_pre_hooks": {}, "_load_state_dict_post_hooks": {}, "_modules": {"im_tokenizer": "ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Identity()\n)", "encoder": "Encoder(\n  (position_enc): PositionalEncoding()\n  (dropout): Dropout(p=0.1, inplace=False)\n  (layer_stack): ModuleList(\n    (0-1): 2 x EncoderLayer(\n      (slf_attn): MultiHeadAttention(\n        (w_qs): Linear(in_features=512, out_features=512, bias=False)\n        (w_ks): Linear(in_features=512, out_features=512, bias=False)\n        (w_vs): Linear(in_features=512, out_features=512, bias=False)\n        (fc): Linear(in_features=512, out_features=512, bias=False)\n        (attention): ScaledDotProductAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n      )\n      (pos_ffn): PositionwiseFeedForward(\n        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n)", "decoder": "Decoder(\n  (layer_stack): ModuleList(\n    (0-1): 2 x DecoderLayer(\n      (enc_attn): MultiHeadAttention(\n        (w_qs): Linear(in_features=512, out_features=512, bias=False)\n        (w_ks): Linear(in_features=512, out_features=512, bias=False)\n        (w_vs): Linear(in_features=512, out_features=512, bias=False)\n        (fc): Linear(in_features=512, out_features=512, bias=False)\n        (attention): ScaledDotProductAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n      )\n      (pos_ffn): PositionwiseFeedForward(\n        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n)", "output_attn": "OutputAttention(\n  (w_qs): Linear(in_features=512, out_features=64, bias=False)\n  (w_ks): Linear(in_features=512, out_features=64, bias=False)\n  (w_vs): Linear(in_features=512, out_features=64, bias=False)\n)"}}, "load_checkpoint": null}